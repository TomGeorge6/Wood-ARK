#!/usr/bin/env python3
"""
Wood-ARK: ARK ETF æŒä»“ç›‘æ§å·¥å…·

ä¸»ç¨‹åºå…¥å£ï¼Œè´Ÿè´£å‘½ä»¤è¡Œå‚æ•°è§£æå’Œæµç¨‹ç¼–æ’ã€‚
"""

import argparse
import sys
import logging
from pathlib import Path

from src.utils import load_config, setup_logging, cleanup_old_logs
from src.fetcher import DataFetcher
from src.analyzer import Analyzer
from src.reporter import ReportGenerator
from src.image_generator import ImageGenerator
from src.notifier import WeChatNotifier
from src.scheduler import Scheduler
from src.summary_analyzer import SummaryAnalyzer
from src.summary_notifier import SummaryNotifier

logger = logging.getLogger(__name__)


def test_webhook_mode(config) -> int:
    """
    æµ‹è¯• Webhook è¿æ¥æ¨¡å¼
    
    Returns:
        é€€å‡ºç ï¼ˆ0 æˆåŠŸï¼Œ1 å¤±è´¥ï¼‰
    """
    logger.info("=== æµ‹è¯• Webhook è¿æ¥ ===")
    
    notifier = WeChatNotifier(
        webhook_url=config.notification.webhook_url,
        max_retries=config.retry.max_retries,
        retry_delays=config.retry.retry_delays
    )
    
    if notifier.test_connection():
        print("âœ… Webhook æµ‹è¯•æˆåŠŸ")
        return 0
    else:
        print("âŒ Webhook æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return 1


def check_missed_mode(config) -> int:
    """
    æ£€æŸ¥ç¼ºå¤±æ•°æ®æ¨¡å¼ï¼ˆä»…æŸ¥çœ‹ï¼Œä¸è¡¥é½ï¼‰
    
    âš ï¸ ç”±äº API é™åˆ¶ï¼Œæ— æ³•è¡¥é½å†å²æ•°æ®ï¼Œæ­¤åŠŸèƒ½ä»…ç”¨äºæŸ¥çœ‹ç¼ºå¤±æƒ…å†µ
    
    Returns:
        é€€å‡ºç ï¼ˆ0 æˆåŠŸï¼Œ1 å¤±è´¥ï¼‰
    """
    logger.info("=== æ£€æŸ¥ç¼ºå¤±æ•°æ® ===")
    
    scheduler = Scheduler(
        data_dir=config.data.data_dir,
        enable_schedule=False
    )
    
    etf_symbols = config.data.etfs
    missed = scheduler.check_missed_dates(etf_symbols, days=7)
    
    if not missed:
        print("âœ… æœªå‘ç°ç¼ºå¤±æ•°æ®")
        return 0
    
    print(f"âš ï¸  å‘ç°ç¼ºå¤±æ•°æ®:")
    for etf, dates in missed.items():
        print(f"  {etf}: {', '.join(dates)}")
    
    print("\nğŸ’¡ æç¤ºï¼š")
    print("   ç”±äº ARKFunds.io API åªèƒ½è·å–å½“æ—¥æ•°æ®ï¼Œæ— æ³•è¡¥é½å†å²ç¼ºå¤±æ•°æ®ã€‚")
    print("   å»ºè®®æ¯å¤©å®šæ—¶è¿è¡Œï¼Œè‡ªç„¶ç´¯ç§¯æ•°æ®ã€‚")
    
    return 0


def run_daily_task(
    config,
    target_date: str = None,
    etf_filter: str = None,
    force: bool = False
) -> int:
    """
    æ‰§è¡Œæ¯æ—¥ä»»åŠ¡
    
    Args:
        config: é…ç½®å¯¹è±¡
        target_date: ç›®æ ‡æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        etf_filter: åªå¤„ç†æŒ‡å®š ETFï¼ˆå¯é€‰ï¼‰
        force: æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œ
    
    Returns:
        é€€å‡ºç ï¼ˆ0 æˆåŠŸï¼Œ1 å¤±è´¥ï¼‰
    """
    logger.info("=== å¼€å§‹æ¯æ—¥ä»»åŠ¡ ===")
    
    # åˆå§‹åŒ–å„æ¨¡å—
    scheduler = Scheduler(
        data_dir=config.data.data_dir,
        enable_schedule=config.schedule.enabled
    )
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿è¡Œ
    if not scheduler.should_run_today(force=force):
        logger.info("ä»Šå¤©ä¸éœ€è¦æ‰§è¡Œä»»åŠ¡")
        return 0
    
    # âš ï¸ ä¸æ‰§è¡Œä»»ä½•è¡¥é½é€»è¾‘
    # åªè·å–ä»Šå¤©çš„æ•°æ®ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§
    
    # è·å–ç›®æ ‡æ—¥æœŸå’Œå¯¹æ¯”æ—¥æœŸ
    target_date = scheduler.get_target_date(target_date)
    comparison_date = scheduler.get_comparison_date(target_date)
    
    logger.info(f"ç›®æ ‡æ—¥æœŸ: {target_date}, å¯¹æ¯”æ—¥æœŸ: {comparison_date}")
    
    # åˆå§‹åŒ–ç»„ä»¶
    fetcher = DataFetcher(config=config)
    
    # 0. è‡ªåŠ¨ä¸‹è½½å†å²æ•°æ®ï¼ˆé¦–æ¬¡è¿è¡Œæˆ–æ•°æ®ä¸è¶³æ—¶ï¼‰
    if config.data.auto_download_history:
        logger.info("[0/6] æ£€æŸ¥å¹¶ä¸‹è½½å†å²æ•°æ®...")
        for etf in config.data.etfs:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ï¼ˆè‡³å°‘ 5 å¤©ï¼‰
            holdings_dir = Path(config.data.data_dir) / "holdings" / etf
            if holdings_dir.exists():
                existing_files = list(holdings_dir.glob("*.csv"))
                if len(existing_files) >= 5:
                    logger.debug(f"{etf} å·²æœ‰ {len(existing_files)} å¤©æ•°æ®ï¼Œè·³è¿‡ä¸‹è½½")
                    continue
            
            logger.info(f"ä¸‹è½½ {etf} å†å²æ•°æ®...")
            fetcher.download_historical_data(etf, days=config.data.history_days)
    
    # 0.5 æ¸…ç†è¿‡æœŸæ•°æ®
    logger.info("[0.5/6] æ¸…ç†è¿‡æœŸæ•°æ®...")
    cleanup_stats = fetcher.cleanup_old_data(retention_days=config.data.retention_days)
    if cleanup_stats:
        total_deleted = sum(s['deleted_count'] for s in cleanup_stats.values())
        logger.info(f"æ¸…ç†å®Œæˆ: åˆ é™¤ {total_deleted} ä¸ªè¿‡æœŸæ–‡ä»¶")
    
    analyzer = Analyzer(threshold=config.analysis.change_threshold)
    
    reporter = ReportGenerator(data_dir=config.data.data_dir)
    
    image_gen = ImageGenerator(data_dir=config.data.data_dir)
    
    notifier = WeChatNotifier(
        webhook_url=config.notification.webhook_url,
        max_retries=config.retry.max_retries,
        retry_delays=config.retry.retry_delays
    )
    
    # å¤„ç†æ¯ä¸ª ETF
    etf_symbols = config.data.etfs
    if etf_filter:
        etf_symbols = [etf_filter]
    
    total_success = 0
    total_failed = 0
    
    # å­˜å‚¨æ‰€æœ‰ETFçš„æŒä»“æ•°æ®ï¼ˆç”¨äºæ±‡æ€»æŠ¥å‘Šï¼‰
    all_current_holdings = {}  # {etf: [dict, ...]}
    all_previous_holdings = {}  # {etf: [dict, ...]}
    all_current_dfs = {}  # {etf: DataFrame}
    all_previous_dfs = {}  # {etf: DataFrame}
    all_analysis_results = {}  # {etf: analysis_result}
    all_etf_images = {}  # {etf: [image_paths]}
    
    for etf in etf_symbols:
        try:
            logger.info(f"\n{'='*50}")
            logger.info(f"å¤„ç† {etf}")
            logger.info(f"{'='*50}")
            
            # 1. è·å–æ•°æ®
            logger.info(f"[1/5] è·å– {etf} æŒä»“æ•°æ®...")
            current_df = fetcher.fetch_holdings(etf, target_date)
            previous_df = fetcher.fetch_holdings(etf, comparison_date)
            
            if current_df is None or previous_df is None:
                logger.error(f"âŒ {etf} æ•°æ®è·å–å¤±è´¥ï¼Œè·³è¿‡")
                total_failed += 1
                continue
            
            # ä¿å­˜åˆ°æœ¬åœ°
            fetcher.save_to_csv(current_df, etf, target_date)
            
            # 2. åˆ†æå˜åŒ–
            logger.info(f"[2/5] åˆ†ææŒä»“å˜åŒ–...")
            analysis_result = analyzer.compare_holdings(
                current_df, previous_df, comparison_date, target_date
            )
            
            # 3. ç”ŸæˆæŠ¥å‘Š
            logger.info(f"[3/5] ç”Ÿæˆ Markdown æŠ¥å‘Š...")
            current_holdings = current_df.to_dict('records')
            markdown = reporter.generate_markdown(
                analysis_result, etf, current_holdings
            )
            
            # ä¿å­˜æŠ¥å‘Š
            report_path = reporter.save_report(markdown, etf, target_date)
            logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
            # 4. ç”Ÿæˆå¯è§†åŒ–é•¿å›¾
            logger.info(f"[4/5] ç”Ÿæˆç»¼åˆæŠ¥å‘Šé•¿å›¾...")
            image_paths = []
            
            try:
                # æå–æ–°å¢è‚¡ç¥¨ä»£ç åˆ—è¡¨
                added_tickers = [h.ticker for h in analysis_result['added']]
                
                # ç”Ÿæˆå•å¼ é•¿å›¾ï¼ˆåŒ…å«æŒä»“è¡¨æ ¼ã€åŸºé‡‘è¶‹åŠ¿ã€Top 10 è¶‹åŠ¿ã€æ–°å¢è‚¡ç¥¨è¶‹åŠ¿ï¼‰
                comprehensive_img = image_gen.generate_comprehensive_report_image(
                    current_holdings, 
                    current_df, 
                    previous_df, 
                    etf, 
                    target_date,
                    added_tickers=added_tickers
                )
                image_paths.append(comprehensive_img)
                logger.info(f"ç»¼åˆæŠ¥å‘Šé•¿å›¾å·²ç”Ÿæˆ: {comprehensive_img}")
            except Exception as e:
                logger.warning(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            
            # ä¿å­˜æ•°æ®ç”¨äºåç»­åˆå¹¶æ¨é€
            all_current_holdings[etf] = current_holdings
            all_previous_holdings[etf] = previous_df.to_dict('records')
            all_current_dfs[etf] = current_df
            all_previous_dfs[etf] = previous_df
            all_analysis_results[etf] = analysis_result
            all_etf_images[etf] = image_paths
            
            logger.info(f"âœ… {etf} å¤„ç†å®Œæˆ")
            total_success += 1
        
        except Exception as e:
            logger.error(f"âŒ å¤„ç† {etf} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            total_failed += 1
            
            # å‘é€é”™è¯¯å‘Šè­¦
            if config.notification.enable_error_alert:
                notifier.send_error_alert(str(e), etf)
    
    # æ±‡æ€»ç»“æœ
    logger.info(f"\n{'='*50}")
    logger.info(f"æ•°æ®å¤„ç†å®Œæˆ: æˆåŠŸ {total_success}, å¤±è´¥ {total_failed}")
    logger.info(f"{'='*50}")
    
    # ========== åˆ†æ‰¹æ¨é€ï¼ˆæ–¹æ¡ˆAï¼šç¨³å®šæ€§æœ€é«˜ï¼‰==========
    if total_success >= 2:  # è‡³å°‘æˆåŠŸ2ä¸ªåŸºé‡‘æ‰æ¨é€
        try:
            logger.info(f"\n{'='*50}")
            logger.info("å¼€å§‹åˆ†æ‰¹æ¨é€ï¼ˆ7æ¡æ¶ˆæ¯ï¼š1æ–‡å­— + 6å›¾ç‰‡ï¼‰")
            logger.info(f"{'='*50}")
            
            # === æ­¥éª¤1ï¼šæ±‡æ€»åˆ†æ ===
            logger.info("[æ­¥éª¤ 1/7] ç”Ÿæˆæ±‡æ€»åˆ†æ...")
            summary_analyzer = SummaryAnalyzer()
            summary_result = summary_analyzer.analyze_all_etfs(
                current_holdings=all_current_holdings,
                previous_holdings=all_previous_holdings
            )
            
            logger.info(f"âœ… æ±‡æ€»åˆ†æå®Œæˆ: {summary_result['statistics']['total_stocks']} åªè‚¡ç¥¨, "
                       f"{summary_result['statistics']['overlapping_count']} åªè·¨åŸºé‡‘é‡å ")
            
            # === æ­¥éª¤2ï¼šç”Ÿæˆå¹¶å‘é€è¶…é•¿æ–‡å­— ===
            logger.info("[æ­¥éª¤ 2/7] ç”Ÿæˆå¹¶å‘é€è¶…é•¿æ–‡å­—æ¶ˆæ¯...")
            
            # 2.1 ç”Ÿæˆæ±‡æ€»æ–‡å­—
            summary_notifier_gen = SummaryNotifier()
            summary_markdown = summary_notifier_gen.generate_wechat_markdown(summary_result)
            
            # 2.2 è¿½åŠ å„åŸºé‡‘æ‘˜è¦
            combined_text_lines = [summary_markdown, "\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"]
            
            for etf in ['ARKK', 'ARKW', 'ARKG', 'ARKQ', 'ARKF']:
                if etf not in all_analysis_results:
                    continue
                
                analysis_result = all_analysis_results[etf]
                etf_text = notifier.generate_etf_wechat_markdown(
                    etf_symbol=etf,
                    date=target_date,
                    prev_date=analysis_result['prev_date'],
                    curr_date=analysis_result['curr_date'],
                    analysis_result=analysis_result
                )
                combined_text_lines.append(etf_text)
                combined_text_lines.append("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
            
            combined_text = '\n'.join(combined_text_lines)
            
            # å‘é€æ–‡å­—æ¶ˆæ¯
            if notifier.send_markdown(combined_text):
                logger.info("âœ… [2/7] æ–‡å­—æ¶ˆæ¯å‘é€æˆåŠŸ")
            else:
                logger.error("âŒ [2/7] æ–‡å­—æ¶ˆæ¯å‘é€å¤±è´¥")
            
            import time
            time.sleep(0.5)  # é¿å…å‘é€è¿‡å¿«
            
            # === æ­¥éª¤3ï¼šç”Ÿæˆå¹¶å‘é€æ±‡æ€»é•¿å›¾ ===
            logger.info("[æ­¥éª¤ 3/7] ç”Ÿæˆå¹¶å‘é€æ±‡æ€»é•¿å›¾...")
            summary_image = image_gen.generate_summary_report_image(
                summary_result, target_date
            )
            
            if notifier.send_image(summary_image):
                logger.info("âœ… [3/7] æ±‡æ€»é•¿å›¾å‘é€æˆåŠŸ")
            else:
                logger.error("âŒ [3/7] æ±‡æ€»é•¿å›¾å‘é€å¤±è´¥")
            
            time.sleep(0.5)
            
            # === æ­¥éª¤4-8ï¼šä¾æ¬¡å‘é€å„åŸºé‡‘é•¿å›¾ ===
            image_success_count = 1 if notifier.send_image(summary_image) else 0
            
            for idx, etf in enumerate(['ARKK', 'ARKW', 'ARKG', 'ARKQ', 'ARKF'], start=4):
                if etf not in all_etf_images or not all_etf_images[etf]:
                    logger.warning(f"[{idx}/7] {etf} æ²¡æœ‰å›¾ç‰‡ï¼Œè·³è¿‡")
                    continue
                
                logger.info(f"[æ­¥éª¤ {idx}/7] å‘é€ {etf} é•¿å›¾...")
                etf_image_path = all_etf_images[etf][0]
                
                if notifier.send_image(etf_image_path):
                    logger.info(f"âœ… [{idx}/7] {etf} é•¿å›¾å‘é€æˆåŠŸ")
                    image_success_count += 1
                else:
                    logger.error(f"âŒ [{idx}/7] {etf} é•¿å›¾å‘é€å¤±è´¥")
                
                time.sleep(0.5)  # é¿å…å‘é€è¿‡å¿«
            
            # === æ±‡æ€»æ¨é€ç»“æœ ===
            logger.info(f"\n{'='*50}")
            logger.info(f"åˆ†æ‰¹æ¨é€å®Œæˆ: å›¾ç‰‡ {image_success_count}/6 æˆåŠŸ")
            logger.info(f"{'='*50}")
            
            # æ ‡è®°æ¨é€çŠ¶æ€ï¼ˆåªè¦æœ‰ä¸€å¼ å›¾å‘é€æˆåŠŸå°±ç®—æˆåŠŸï¼‰
            push_success = image_success_count > 0
            
            for etf in all_analysis_results.keys():
                scheduler.mark_pushed(etf, target_date, success=push_success)
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æ‰¹æ¨é€æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            if config.notification.enable_error_alert:
                notifier.send_error_alert(f"åˆ†æ‰¹æ¨é€å¤±è´¥: {e}", "ALL")
    else:
        logger.info("è·³è¿‡æ¨é€ï¼ˆæˆåŠŸçš„åŸºé‡‘æ•°é‡ä¸è¶³ï¼‰")
    
    return 0 if total_failed == 0 else 1


def backfill_mode(config, days: int = 90) -> int:
    """
    âš ï¸ æ­¤åŠŸèƒ½å·²åºŸå¼ƒ
    
    åŸå› ï¼šARKFunds.io API åªèƒ½è·å–å½“æ—¥æ•°æ®ï¼Œæ— æ³•è¡¥é½å†å²æ•°æ®
    """
    logger.warning("âš ï¸ backfill åŠŸèƒ½å·²åºŸå¼ƒï¼ˆAPI é™åˆ¶ï¼šåªèƒ½è·å–å½“æ—¥æ•°æ®ï¼‰")
    print("âŒ æ­¤åŠŸèƒ½å·²åºŸå¼ƒ")
    print("ğŸ’¡ åŸå› ï¼šARKFunds.io API åªèƒ½è·å–å½“æ—¥æ•°æ®ï¼Œæ— æ³•è¡¥é½å†å²æ•°æ®")
    print("ğŸ’¡ å»ºè®®ï¼šæ¯å¤©å®šæ—¶è¿è¡Œï¼Œè‡ªç„¶ç´¯ç§¯æ•°æ®")
    return 1

def backfill_mode_deprecated(config, days: int = 90) -> int:
    """
    è¡¥å……å†å²æ•°æ®æ¨¡å¼
    
    Args:
        config: é…ç½®å¯¹è±¡
        days: è¡¥å……å¤©æ•°
    
    Returns:
        é€€å‡ºç ï¼ˆ0 æˆåŠŸï¼Œ1 å¤±è´¥ï¼‰
    """
    logger.info(f"=== è¡¥å……å†å²æ•°æ®ï¼ˆè¿‘ {days} å¤©ï¼‰===")
    
    fetcher = DataFetcher(config=config)
    etf_symbols = config.data.etfs
    
    total_success = 0
    
    for etf in etf_symbols:
        logger.info(f"ä¸‹è½½ {etf} å†å²æ•°æ®...")
        count = fetcher.download_historical_data(etf, days=days)
        total_success += count
        logger.info(f"âœ… {etf}: æ–°å¢ {count} ä¸ªæ–‡ä»¶")
    
    logger.info(f"âœ… å†å²æ•°æ®è¡¥å……å®Œæˆ: æ€»è®¡æ–°å¢ {total_success} ä¸ªæ–‡ä»¶")
    return 0


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Wood-ARK: ARK ETF æŒä»“å˜åŒ–ç›‘æ§å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py                    # è‡ªåŠ¨æ¨¡å¼ï¼ˆå·¥ä½œæ—¥æ‰§è¡Œï¼‰
  python main.py --manual           # æ‰‹åŠ¨æ¨¡å¼ï¼ˆå¼ºåˆ¶æ‰§è¡Œï¼‰
  python main.py --date 2025-01-15  # æŒ‡å®šæ—¥æœŸ
  python main.py --check-missed     # æ£€æŸ¥ç¼ºå¤±æ•°æ®ï¼ˆä»…æŸ¥çœ‹ï¼Œä¸è¡¥é½ï¼‰
  python main.py --test-webhook     # æµ‹è¯• Webhook
        """
    )
    
    parser.add_argument(
        '--manual',
        action='store_true',
        help='æ‰‹åŠ¨æ¨¡å¼ï¼šå¼ºåˆ¶æ‰§è¡Œï¼ˆå¿½ç•¥å·¥ä½œæ—¥æ£€æŸ¥ï¼‰'
    )
    
    parser.add_argument(
        '--date',
        type=str,
        help='æŒ‡å®šæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰æ—¥æœŸ'
    )
    
    parser.add_argument(
        '--check-missed',
        action='store_true',
        help='æ£€æŸ¥å¹¶è¡¥å……æœ€è¿‘ 7 å¤©ç¼ºå¤±çš„æ•°æ®'
    )
    
    parser.add_argument(
        '--test-webhook',
        action='store_true',
        help='æµ‹è¯•ä¼ä¸šå¾®ä¿¡ Webhook è¿æ¥'
    )
    
    parser.add_argument(
        '--etf',
        type=str,
        help='åªå¤„ç†æŒ‡å®š ETFï¼ˆå¦‚ ARKKï¼‰'
    )
    
    parser.add_argument(
        '--backfill',
        action='store_true',
        help='[å·²åºŸå¼ƒ] è¡¥å……å†å²æ•°æ®ï¼ˆAPI é™åˆ¶ï¼Œæ— æ³•ä½¿ç”¨ï¼‰'
    )
    
    parser.add_argument(
        '--days',
        type=int,
        default=90,
        help='[å·²åºŸå¼ƒ] è¡¥å……å†å²æ•°æ®çš„å¤©æ•°'
    )
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = load_config()
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(
            log_dir=config.data.log_dir,  # log_dir åœ¨ data é…ç½®ä¸­
            log_level=config.log.level
        )
        
        # æ¸…ç†æ—§æ—¥å¿—
        cleanup_old_logs(
            log_dir=config.data.log_dir,  # log_dir åœ¨ data é…ç½®ä¸­
            retention_days=config.log.retention_days
        )
        
        logger.info("Wood-ARK å¯åŠ¨")
        logger.info(f"å‚æ•°: {vars(args)}")
        
        # æ ¹æ®å‚æ•°é€‰æ‹©æ‰§è¡Œæ¨¡å¼
        if args.test_webhook:
            exit_code = test_webhook_mode(config)
        
        elif args.check_missed:
            exit_code = check_missed_mode(config)
        
        elif args.backfill:
            exit_code = backfill_mode(config, days=args.days)
        
        else:
            # æ­£å¸¸æ‰§è¡Œæ¨¡å¼
            exit_code = run_daily_task(
                config=config,
                target_date=args.date,
                etf_filter=args.etf,
                force=args.manual
            )
        
        logger.info(f"Wood-ARK é€€å‡ºï¼Œé€€å‡ºç : {exit_code}")
        sys.exit(exit_code)
    
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(130)
    
    except Exception as e:
        logger.error(f"ç¨‹åºå‘ç”Ÿæœªå¤„ç†å¼‚å¸¸: {e}", exc_info=True)
        sys.exit(1)


if __name__ == '__main__':
    main()
